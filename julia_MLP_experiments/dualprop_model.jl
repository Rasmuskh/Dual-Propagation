struct dualprop_Dense{F1, M1<:AbstractMatrix, M2, V1, V2}
    Wâ‚–â‚‹â‚::M1
    Wâ‚–::M2
    bâ‚–â‚‹â‚::V1
    bâ‚–::V2
    Ïƒ::F1 # activation function
    function dualprop_Dense(Wâ‚–â‚‹â‚::M1, Wâ‚–::M2, bâ‚–â‚‹â‚::V1, bâ‚–::V2, Ïƒ::F1 = identity) where {M1<:AbstractMatrix, M2, V1, V2, F1}
        new{F1, M1, M2, V1, V2}(Wâ‚–â‚‹â‚, Wâ‚–, bâ‚–â‚‹â‚, bâ‚–, Ïƒ)
    end
end

Flux.@functor dualprop_Dense

# pretty printing stuff ðŸŒˆ
function Base.show(io::IO, l::dualprop_Dense)
    print(io, "dualprop_Dense(size(Wâ‚–â‚‹â‚)=", size(l.Wâ‚–â‚‹â‚))
    l.Wâ‚– == false || print(io, ", size(Wâ‚–)=", size(l.Wâ‚–))
    l.bâ‚–â‚‹â‚ == false || print(io, ", size(bâ‚–â‚‹â‚)=", size(l.bâ‚–â‚‹â‚))
    l.bâ‚– == false || print(io, ", size(bâ‚–)=", size(l.bâ‚–))
    l.Ïƒ == identity || print(io, ", Ïƒ=", l.Ïƒ)
    print(io, ")")
end

Flux.trainable(a::dualprop_Dense) = (a.Wâ‚–â‚‹â‚, a.Wâ‚–, a.bâ‚–â‚‹â‚, a.bâ‚–)

# Feed-forward inference pass. The recurrent pass below is equivalent to this FF pass when (zâºâ‚–â‚Šâ‚, zâ»â‚–â‚Šâ‚) are both zero=#
function (a::dualprop_Dense)(x::AbstractVecOrMat)
    return a.Ïƒ.(a.Wâ‚–â‚‹â‚*x .+ a.bâ‚–â‚‹â‚)
end

(a::dualprop_Dense)(x::AbstractArray) = reshape(a(reshape(x, size(x,1), :)), :, size(x)[2:end]...)

# Recurrent inference pass. Infer latent state (zâ»â‚–, zâºâ‚–) given neighbouring layers activations (zâ»â‚–â‚‹â‚, zâºâ‚–â‚‹â‚) and (zâºâ‚–â‚Šâ‚, zâºâ‚–â‚Šâ‚)
function (a::dualprop_Dense)(zâºâ‚–â‚‹â‚::AbstractVecOrMat, zâºâ‚–::AbstractVecOrMat, zâºâ‚–â‚Šâ‚::AbstractVecOrMat, zâ»â‚–â‚‹â‚::AbstractVecOrMat, zâ»â‚–::AbstractVecOrMat, zâ»â‚–â‚Šâ‚::AbstractVecOrMat, Î³)
    FF = a.Wâ‚–â‚‹â‚ * (zâºâ‚–â‚‹â‚ + zâ»â‚–â‚‹â‚) .+ 2f0 * a.bâ‚–â‚‹â‚
    FB = Î³*transpose(a.Wâ‚–)*(zâºâ‚–â‚Šâ‚ - zâ»â‚–â‚Šâ‚)
    zâºâ‚– .= a.Ïƒ.(0.5f0 * (FF + FB))
    zâ»â‚– .= a.Ïƒ.(0.5f0 * (FF - FB))
    return zâºâ‚–, zâ»â‚–
end

# Compute energy of dualprop_Dense layer given activations zâ‚–â‚‹â‚ and zâ‚–
function energy(l::dualprop_Dense, zâ‚–, zâ‚–â‚‹â‚)
    batchsize = size(zâ‚–â‚‹â‚, 2)
    return 0.5f0*sum(abs2, l.Wâ‚–â‚‹â‚*zâ‚–â‚‹â‚  .+ l.bâ‚–â‚‹â‚ .- zâ‚–)/batchsize
end

function contr_loss_layer(l::dualprop_Dense, zâºâ‚–::AbstractVecOrMat, zâºâ‚–â‚‹â‚::AbstractVecOrMat, zâ»â‚–::AbstractVecOrMat, zâ»â‚–â‚‹â‚::AbstractVecOrMat)
    return energy(l, zâºâ‚–, zâºâ‚–â‚‹â‚) + energy(l, zâºâ‚–, zâ»â‚–â‚‹â‚) - energy(l, zâ»â‚–, zâºâ‚–â‚‹â‚) - energy(l, zâ»â‚–, zâ»â‚–â‚‹â‚)
end

function contr_loss_network(model, batchsize, zâº, zâ», pred, y, Î³)
    ls = 0f0
    for i=1:length(model)
        ls += Î³*contr_loss_layer(model[i], zâº[i+1], zâº[i], zâ»[i+1], zâ»[i])
    end

    # ls += sum(abs2, 0.5f0*model[end].Wâ‚–â‚‹â‚*(zâº[end-1] + zâ»[end-1])- y)/batchsize
    # ls += sum(diag(transpose(zâº[end] - zâ»[end])*(0.5f0*(zâº[end] + zâ»[end]) - y)))
    # ls += sum(diag(transpose(zâº[end] - zâ»[end])*(zâº[end] - zâ»[end])))
    Î” = pred - y
    ls += sum(diag(transpose(2*Î³*Î”)*Î”))/batchsize

    return ls
end
    
function build_dualprop_MLP(N=[784, 1024, 512, 256, 10])
    W = [Flux.glorot_uniform(N[k+1], N[k]) for k=1:length(N)-1]
    b = [zeros(Float32, N[k+1]) for k=1:length(N)-1]
    hiddenlayers = [dualprop_Dense(W[k], W[k+1], b[k], b[k+1], relu) for k=1:length(N)-2]
    outputlayer = dualprop_Dense(W[end], false, b[end], false, identity)
    model = Chain(hiddenlayers..., outputlayer)
    return model
end    
